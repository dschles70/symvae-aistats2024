# symvae-aistats2024

Implementations of experiments from the paper (links/title ... follow).

The repository contains three directories: `mnist-ladder`, `mnist-reverse` and `fmnist-ladder`. Thereby, `...-ladder` means the direct encoder factorization order as in ladder-VAEs, whereas `...-reverse` corresponds to the reverse encoder factorization order as in the Wake-Sleep algorithm, `mnist`/`fmnist` denote the dataset used.

The code is pretty similar in all three cases, hence, we shortly describe the usage right here. For the description of the learning methods we refer to the paper. Implementation details can be found in the code.

### 1) Learning the models

In order to learn models, go to the desired directory and type e.g. \
`python main_sym.py --call_prefix 0 --nz0 30 --nz1 100 --stepsize 1e-4 --niterations 200000` \
for symmetric learning and e.g. \
`python main_elbo.py --call_prefix 1 --nz0 30 --nz1 100 --stepsize 1e-4 --niterations 200000` \
for learning by ELBO maximization. In these examples we learn a HVAE with 30 bits for $z_0$ and 100 bits for $z_1$, the learning rate is 1e-4, we want to perform 200k gradient update steps. Note that experiments in the paper were performed with 1M iterations. It takes however quite a long time to work. Usually, useful results are obtained much more quickly. Hence, in order to illustrate the usage 200k iterations are enough. The option `--call_prefix` is a tag (can be an arbitrary string), which identifies the learned model as well as some additional produced results (see below). This tag can be used for example for generation, continuing the learning from a checkpoint etc.

The results of a call as above are stored in three sub-directories (created if necessary): `images`, `logs` and `models`. The names are self-speaking. The course of the learning process (for example, losses, illustrating images etc.) can be watched using the corresponding notebooks `plot_sym.ipynb` and `plot_elbo.ipynb`.

### 2) Generating images

After a model is trained, the images can be generated by invoking \
`python generate.py --call_prefix 0 --nz0 30 --nz1 100` \
The resulting images are stored in `./images/generated_sh_0.png` and `./images/generated_sta_0.png` (for the model tagged `0`, i.e. symmetric learning in this example) for images generated from random codes and from limiting distribution respectively.

### 3) Computing FID-scores

In order to compute FID-scores, first we need to generate lots of images and store them as image-files. This can be done by the following command: \
`python generate_data.py --call_prefix 0 --mode sym --nz0 30 --nz1 100` \
for symmetric learning or \
`python generate_data.py --call_prefix 1 --mode elbo --nz0 30 --nz1 100` \
for ELBO-maximization. After that a further subdirectory called `generated_data` is created and filled with generated images, the option `--mode` defines the name of subdirectory of `generated_data` the corresponding images are stored in.

After the necessary image sets are generated, FID-scores can be computed by e.g. \
`python -m pytorch_fid --device cuda:0 generated_data/elbo/images/ generated_data/elbo/lim_images/` \
(we used FID implementation from [here](https://github.com/mseitzer/pytorch-fid), so please install it first). In the above example we compute FID between the original dataset and images generated from limiting distribution for the model learned by ELBO.
___

Finally, after performing all steps as described above for ladder VAE on MNIST we get the following FID scores:

|             | Random code | Limiting |
| ---         | ---         | ---      |
__ELBO__      | 3.2227      | 36.6440  |
__Symmetric__ | 1.6543      | 4.1015   |
| | |  |

For MNIST with the reverse encoder factorization:

|             | Random code | Limiting |
| ---         | ---         | ---      |
__ELBO__      | 5.2889      | 14.2256  |
__Symmetric__ | 1.4746      | 4.9559  |
| | |  |
